{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rr1d-kHohxGL"
   },
   "outputs": [],
   "source": [
    "#!pip install gym[atari,accept-rom-license]==0.25.2\n",
    "import sys, os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IGCa_JQeiy1F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.spaces import Box\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        transform = torchvision.transforms.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
    "                                                     torchvision.transforms.Normalize(0, 255)])\n",
    "        return transforms(observation).squeeze(0)\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
    "        # ...\n",
    "        \n",
    "        return torch.tensor(state), torch.tensor(next_state), torch.tensor(action), torch.tensor(reward), torch.tensor(done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LOCiUgfBjJYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stacked frames:  4\n",
      "Resized observation space dimensionality:  84 84\n",
      "Number of available actions by the agent:  6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "\n",
    "env_rendering = False    # Set to False while training your model on Colab\n",
    "testing_mode = False\n",
    "test_model_directory = './your_saved_model.pth.tar'\n",
    "\n",
    "# Create and preprocess the Space Invaders environment\n",
    "if env_rendering:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space=False)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "image_stack, h, w = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print('Number of stacked frames: ', image_stack)\n",
    "print('Resized observation space dimensionality: ', h, w)\n",
    "print('Number of available actions by the agent: ', num_actions)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 61\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters (to be modified)\n",
    "batch_size = 32\n",
    "alpha = 0.00025\n",
    "gamma = 0.95\n",
    "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
    "buffer = ExperienceReplayMemory(20000)\n",
    "burn_in_phase = 20000\n",
    "sync_target = 30000\n",
    "max_train_frames = 10000\n",
    "max_train_episodes = 100000\n",
    "max_test_episodes = 1\n",
    "curr_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NNoUgNjujqRX"
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    return torch.tensor(x.__array__()).float()\n",
    "\n",
    "\n",
    "class DeepQNet(torch.nn.Module):\n",
    "    def __init__(self, h, w, image_stack, num_actions):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        # TODO: create a convolutional neural network\n",
    "        # taken from torch-demo\n",
    "        # TODO: find out how to properly use the conctructor-arguments here\n",
    "        \n",
    "        # Grayscale image has one channel only\n",
    "        n_input_channes = 1\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=n_input_channes, out_channels=5, kernel_size=(3, 3))\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=5, out_channels=10, kernel_size=(3, 3))\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3))\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        # fc: Full Connection\n",
    "        self.fc1 = torch.nn.Linear(20 * 2 * 2, 40)\n",
    "        self.fc2 = torch.nn.Linear(40, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass from the neural network\n",
    "        # taken from torch-demo\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)     # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
    "\n",
    "# Rich: require_gradients could be used to freeze parameters from being changed. Is this required for \n",
    "# one of the networks? Example code for freezing:\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "    \n",
    "    \n",
    "online_dqn = DeepQNet(h, w, image_stack, num_actions)\n",
    "target_dqn = copy.deepcopy(online_dqn)\n",
    "online_dqn.to(device)\n",
    "target_dqn.to(device)\n",
    "\n",
    "\n",
    "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
    "# TODO: decide if online or target parameters to be passed to Adam\n",
    "optimizer = torch.optim.Adam(target_dqn.parameters())\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RO21LQJ6j0WC"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3400882572.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_20743/3400882572.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    if rng.r\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "rng = np.default_rng()\n",
    "def policy(state, is_training):\n",
    "    global eps\n",
    "    state = convert(state).unsqueeze(0).to(device)\n",
    "    # TODO: Implement an epsilon-greedy policy\n",
    "    if is_training and (rng.rand() <= eps):\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return online_dqn.forward(state).argmax()\n",
    "\n",
    "\n",
    "def compute_loss(state, action, reward, next_state, done):\n",
    "    state = convert(state).to(device)\n",
    "    next_state = convert(next_state).to(device)\n",
    "    action = action.to(device)\n",
    "    reward = reward.to(device)\n",
    "    done = done.to(device)\n",
    "    \n",
    "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_episode(curr_step, buffer, is_training):\n",
    "    global eps\n",
    "    global target_dqn\n",
    "    episode_reward, episode_loss = 0, 0.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(max_train_frames):\n",
    "        action = policy(state, is_training)\n",
    "        curr_step += 1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if is_training:\n",
    "            buffer.store(state, next_state, action, reward, done)\n",
    "\n",
    "            if curr_step > burn_in_phase:\n",
    "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
    "\n",
    "                if curr_step % sync_target == 0:\n",
    "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
    "                    # ...\n",
    "                    pass\n",
    "\n",
    "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                episode_loss += loss.item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                episode_loss += compute_loss(state, action, reward, next_state, done).item()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEGSGYsQj8Wh"
   },
   "outputs": [],
   "source": [
    "def update_metrics(metrics, episode):\n",
    "    for k, v in episode.items():\n",
    "        metrics[k].append(v)\n",
    "\n",
    "\n",
    "def print_metrics(it, metrics, is_training, window=100):\n",
    "    reward_mean = np.mean(metrics['reward'][-window:])\n",
    "    loss_mean = np.mean(metrics['loss'][-window:])\n",
    "    mode = \"train\" if is_training else \"test\"\n",
    "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(curr_step, eps, train_metrics):\n",
    "    save_dict = {'curr_step': curr_step, \n",
    "                 'train_metrics': train_metrics, \n",
    "                 'eps': eps,\n",
    "                 'online_dqn': online_dqn.state_dict(), \n",
    "                 'target_dqn': target_dqn.state_dict()}\n",
    "    torch.save(save_dict, './your_saved_model.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdhWcoW-kyuF"
   },
   "outputs": [],
   "source": [
    "if testing_mode:\n",
    "    # TODO: Load your saved online_dqn model for evaluation\n",
    "    # ...\n",
    "    test_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_test_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
    "        update_metrics(test_metrics, episode_metrics)\n",
    "        print_metrics(it + 1, test_metrics, is_training=False)\n",
    "else:\n",
    "    train_metrics = dict(reward=[], loss=[])\n",
    "    for it in range(max_train_episodes):\n",
    "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
    "        update_metrics(train_metrics, episode_metrics)\n",
    "        if curr_step > burn_in_phase and eps > min_eps:\n",
    "            eps *= eps_decay\n",
    "        if it % 50 == 0:\n",
    "            print_metrics(it, train_metrics, is_training=True)\n",
    "            save_checkpoint(curr_step, eps, train_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy7C4qfWkzXb"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot your train_metrics and test_metrics\n",
    "# ...\n",
    "env = gym.make(\"ALE/Boxing-v5\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
